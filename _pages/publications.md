---
title: "Publications"
permalink: /publications/
author_profile: true
---

You can also find my articles on my <u><a href="https://scholar.google.com/citations?user=lmUOLU8AAAAJ&hl=ja" target="_blank">
Google Scholar profile</a>.</u>

Journal
======
* <u>M. Cai</u>, J. Kezierbieke, X. Zhong, and H. Chen, &quot;Uncertainty-aware and class-balanced domain adaptation for object detection in driving scenes,&quot; <i>IEEE Transactions on Intelligent Transportation Systems (**T-ITS**)</i>, DOI:10.1109/TITS.2024.3413813, 2024.
[[paper]](https://ieeexplore.ieee.org/document/10570076)
* G. Duan, H. Liu, <u>M. Cai</u>, J. Sun, and H. Chen, &quot;MaDroid: A maliciousness-aware multifeatured dataset for detecting Android malware,&quot; <i>Computers & Security</i>, 2024.
[[paper]](https://www.sciencedirect.com/science/article/abs/pii/S0167404824002748)
* G. Duan, Y. Fu, <u>M. Cai</u>, H. Chen, and J. Sun, &quot;DongTing: a large-scale dataset for anomaly detection of the Linux kernel,&quot; <i>Journal of Systems and Software</i>, 2023.
[[paper]](https://www.sciencedirect.com/science/article/abs/pii/S0164121223001401)
* C. Xue, X. Zhong, <u>M. Cai</u>, H. Chen, and W. Wang, &quot;Audio-visual event localization by learning spatial and semantic co-attention,&quot; <i>IEEE Transactions on Multimedia (**TMM**)</i>, DOI:10.1109/TMM.2021.3127029, 2021. 
* H. Yu, <u>M. Cai</u>, Y. Liu, and F. Lu, &quot;First- and third-person video co-analysis by learning spatial-temporal joint attention,&quot; <i>IEEE Transactions on Pattern Analysis and Machine Intelligence (**TPAMI**)</i>, DOI:10.1109/TPAMI.2020.3030048, 2020. 
* Y. Huang, <u>M. Cai</u>, Z. Li, F. Lu and Y. Sato, &quot;Mutual context network for jointly estimating egocentric gaze and action,&quot; <i>IEEE Transactions
on Image Processing (**TIP**)</i>, DOI:10.1109/TIP.2020.3007841, 2020.
* Y. Huang, <u>M. Cai</u>, and Y. Sato, &quot;An ego-vision system for discovering human joint attention,&quot; <i>IEEE Transactions
on Human-Machine Systems (**THMS**)</i>, DOI:10.1109/THMS.2020.2965429, 2020.
[[project]](https://cai-mj.github.io/project/joint_attention_discovery)
* <u>M. Cai</u>, F. Lu, and Y. Gao, &quot;Desktop action recognition from first-person point-of-view,&quot; <i>IEEE Transactions on Cybernetics (**TCYB**)</i>, DOI:10.1109/TCYB.2018.2806381, 2018.
[[preprint]](/files/CLG_TCYB2018.pdf)
* <u>M. Cai</u>, K. Kitani, and Y. Sato, &quot;An ego-vision system for hand grasp analysis,&quot; <i>IEEE Transactions
on Human-Machine Systems (**THMS**)</i>, vol. 47, no. 4, pp. 524â€“535, 2017.
[[project]](https://cai-mj.github.io/project/visual_grasp_analysis)
[[preprint]](/files/CKS_THMS2017.pdf)

International Conference
======
* H. Huang, H. Yu, D. Liu, H. Chen and <u>M. Cai</u>, &quot;Egocentric speaker diarization with vision-guided clustering and adaptive speech re-detection,&quot; <i>ICASSP</i>, 2025.  
* X. Ren, J. Luo, X. Zhong, and <u>M. Cai</u>, &quot;Emotion-aware audio-driven face animation via contrastive feature disentanglement,&quot; <i>INTERSPEECH</i>, 2023.
[[paper]](https://www.isca-archive.org/interspeech_2023/ren23_interspeech.pdf)
* Z. Liao, F. Xiong, J. Luo, <u>M. Cai</u>, ES Chng, J. Feng, and X. Zhong, &quot;Blind estimation of room impulse response from monaural reverberant speech with segmental generative neural network,&quot; <i>INTERSPEECH</i>, 2023.
[[paper]](https://www.isca-archive.org/interspeech_2023/liao23_interspeech.pdf)
* H. Jiang, J. Hu, D. Liu, J. Xiong, and <u>M. Cai</u>, &quot;Driversonar: Fine-grained dangerous driving detection using active sonar,&quot; <i>Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies (**UbiComp**)</i>, 2021.
[[paper]](https://drive.google.com/file/d/1PofWgWgkGTikuS3xVgauufmHBrHvMKav/view)
* <u>M. Cai</u>, F. Lu, and Y. Sato, &quot;Generalizing hand segmentation in egocentric videos with uncertainty-guided model adaptation,&quot; <i>IEEE Conference on Computer Vision and Pattern Recognition (**CVPR**)</i>, 2020. (<font color="blue">acceptance rate: 22%</font>)
[[project]](https://cai-mj.github.io/project/egocentric_hand_segmentation)
* Z. Li, Y. Huang, <u>M. Cai</u>, and Y. Sato, &quot;Manipulation-skill assessment from videos with spatial attention network,&quot; <i>International Conference on Computer Vision Workshop (**ICCVW**)</i>, 2019.  
[[Arxiv preprint]](https://arxiv.org/abs/1901.02579)
* H. Yu, <u>M. Cai</u>, Y. Liu, and F. Lu, &quot;What I see is what you see: joint attention learning for first and third person video co-analysis,&quot; <i>ACM International Conference on Multimedia (**ACM MM**)</i>, 2019. (<font color="blue">acceptance rate: 26.8%</font>)  
[[Arxiv preprint]](https://arxiv.org/abs/1904.07424)
* Y. Huang, <u>M. Cai</u>, Z. Li, and Y. Sato, &quot;Predicting gaze in egocentric videos by learning task-dependent attention transition,&quot; <i>Proceedings of European Conference on Computer Vision (**ECCV**)</i>, Sep 2018. (<font color="blue">oral presentation, acceptance rate: 2.4%</font>)  
[[project]](https://cai-mj.github.io/project/egocentric_gaze_prediction)
[[Arxiv preprint]](/files/HCLS_eccv_arxiv2018.pdf)
* Y. Huang, <u>M. Cai</u>, H. Kera, R. Yonetani, K. Higuchi, and Y. Sato, &quot;Temporal localization and spatial segmentation of joint attention in multiple first-person videos,&quot; <i>Proceedings of IEEE International Conference on Computer Vision Workshop (**ICCVW**)</i>, pp. 2313-2321, Oct 2017.  
[[project]](https://cai-mj.github.io/project/joint_attention_discovery)
[[paper]](/files/HC_ICCVW2017.pdf)
[[poster]](/files/HC_ICCVW2017_poster.pdf)
* <u>M. Cai</u>, K. Kitani, and Y. Sato, &quot;Understanding hand-object manipulation with grasp types and object attributes,&quot; <i>Proceedings of Robotics: Science and Systems (**RSS**)</i>, XII.034, pp. 1-10, June 2016. (<font color="blue">acceptance rate: 20%</font>)  
[[project]](https://cai-mj.github.io/project/hand_manipulation_understanding)
[[paper]](/files/CKS_RSS2016.pdf)
* <u>M. Cai</u>, K. Kitani, and Y. Sato, &quot;A scalable approach for understanding the visual structures of hand grasps,&quot; <i>Proceedings of IEEE International Conference on Robotics and Automation (**ICRA**)</i>, pp. 1360-1366, May 2015.  
[[project]](https://cai-mj.github.io/project/visual_grasp_analysis)
[[paper]](/files/CKS_ICRA2015.pdf)

Domestic Conference
======
* Z. Li, Y. Huang, <u>M. Cai</u>, and Y. Sato, &quot;Pairwise performance assessment using deep ranking,&quot; <i>Meeting on Image Recognition and Understanding (**MIRU**)</i>, extended abstract, Aug 2018.
* Y. Huang, <u>M. Cai</u>, Z. Li, and Y. Sato, &quot;Egocentric gaze prediction using task-dependent attention transition,&quot; <i>Meeting on Image Recognition and Understanding (**MIRU**)</i>, extended abstract, Aug 2018.
* Y. Huang, <u>M. Cai</u>, H. Kera, R. Yonetani, K. Higuchi, and Y. Sato, &quot;Spatial-temporal segmentation of joint attention in multiple first-person videos,&quot; <i>Meeting on Image Recognition and Understanding (**MIRU**)</i>, extended abstract, Aug 2017.
* <u>M. Cai</u>, K.M. Kitani, and Y. Sato, &quot;Hand skeleton pruning based on contour partition with fingertip
detection,&quot; <i>Meeting on Image Recognition and Understanding (**MIRU**)</i>, extended abstract, Nov 2014.

Technical Report
======
* <u>M. Cai</u>, K.M. Kitani, and Y. Sato, &quot;Studying mutual context of grasp types and object attributes in hand manipulation activities,&quot; <i>IEICE technical report</i>, vol.116 no.208, pp. 105-112, Sep 2016.
* <u>M. Cai</u>, K.M. Kitani, and Y. Sato, &quot;Discovering appearance-based grasp structures with wearable cameras,&quot; <i>IEICE technical report</i>, vol.114 no.351, pp. 49-54, Nov 2014.
